{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6729ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 18:10:24,278 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-17 18:10:24,366 - INFO - Going to convert document batch...\n",
      "2025-10-17 18:10:24,368 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 4f2edc0f7d9bb60b38ebfecf9a2609f5\n",
      "2025-10-17 18:10:24,378 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-17 18:10:24,380 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-17 18:10:24,391 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-17 18:10:24,394 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting fintbx.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 18:10:25,289 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-10-17 18:10:25,318 - INFO - Accelerator device: 'mps'\n",
      "2025-10-17 18:10:26,927 - INFO - Accelerator device: 'mps'\n",
      "2025-10-17 18:10:27,669 - INFO - Processing document fintbx.pdf\n",
      "2025-10-17 18:48:33,408 - INFO - Finished converting document fintbx.pdf in 2289.09 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to: /Users/RiyanshiKedia/Documents/GitHub/project-aurelia/DATA/outputs/simple/fintbx.md\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Run it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m result = \u001b[43mparse_pdf_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/RiyanshiKedia/Documents/GitHub/project-aurelia/DATA/fintbx.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/RiyanshiKedia/Documents/GitHub/project-aurelia/DATA/outputs/simple\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m    \u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mparse_pdf_simple\u001b[39m\u001b[34m(pdf_path, output_dir)\u001b[39m\n\u001b[32m     25\u001b[39m json_path = output_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path.stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43mjson\u001b[49m.dump(result.document.export_to_dict(), f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Stats\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# %% ULTRA-SIMPLIFIED: Let Docling do EVERYTHING\n",
    "from docling.document_converter import DocumentConverter\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_pdf_simple(pdf_path: Path, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Dead simple parsing - let Docling handle it all!\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Converting {pdf_path.name}...\")\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(str(pdf_path))\n",
    "    \n",
    "    # Export to Markdown - Docling includes captions automatically!\n",
    "    markdown = result.document.export_to_markdown()\n",
    "    \n",
    "    # Save\n",
    "    md_path = output_dir / f\"{pdf_path.stem}.md\"\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown)\n",
    "    \n",
    "    print(f\"✓ Saved to: {md_path}\")\n",
    "    \n",
    "    # Also save as JSON for structured access\n",
    "    json_path = output_dir / f\"{pdf_path.stem}.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result.document.export_to_dict(), f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved to: {json_path}\")\n",
    "    \n",
    "    # Stats\n",
    "    doc = result.document\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Pictures: {len(doc.pictures)}\")\n",
    "    print(f\"  Tables: {len(doc.tables)}\")\n",
    "    print(f\"  Text items: {len(doc.texts)}\")\n",
    "    \n",
    "    # Check captions\n",
    "    pics_with_captions = sum(1 for pic in doc.pictures if pic.caption_text(doc=doc))\n",
    "    tables_with_captions = sum(1 for tbl in doc.tables if tbl.caption_text(doc=doc))\n",
    "    \n",
    "    print(f\"\\nCaptions:\")\n",
    "    print(f\"  Pictures with captions: {pics_with_captions}/{len(doc.pictures)} ({pics_with_captions/len(doc.pictures)*100:.1f}%)\")\n",
    "    print(f\"  Tables with captions: {tables_with_captions}/{len(doc.tables)} ({tables_with_captions/len(doc.tables)*100:.1f}%)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run it\n",
    "result = parse_pdf_simple(\n",
    "    pdf_path=Path(\"./data/fintbx.pdf\"),\n",
    "    output_dir=Path(\"./data/outputs/simple\")    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted to JSON: /Users/RiyanshiKedia/Documents/GitHub/project-aurelia/DATA/outputs/simple/fintbx_structured.json\n",
      "  Sections: 10203\n",
      "  Total words: 813913\n"
     ]
    }
   ],
   "source": [
    "# %% PHASE A FINAL - Ready for Phase B Chunking\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.types.doc import TextItem, TableItem, PictureItem\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Parse\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE A: PARSE & NORMALIZE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_path = Path(\"/Users/RiyanshiKedia/Documents/GitHub/project-aurelia/DATA/fintbx_ex.pdf\")\n",
    "output_dir = Path(\"../DATA/outputs/phase_a/artifacts\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(str(pdf_path))\n",
    "doc = result.document\n",
    "\n",
    "# Extract blocks\n",
    "blocks = []\n",
    "block_id = 0\n",
    "section_stack = []\n",
    "\n",
    "for item, level in doc.iterate_items():\n",
    "    \n",
    "    # Metadata\n",
    "    page, bbox = None, None\n",
    "    if hasattr(item, 'prov') and item.prov:\n",
    "        prov = item.prov[0]\n",
    "        page = getattr(prov, 'page_no', None)\n",
    "        if hasattr(prov, 'bbox'):\n",
    "            b = prov.bbox\n",
    "            bbox = {\"x0\": float(b.l), \"y0\": float(b.t), \"x1\": float(b.r), \"y1\": float(b.b)}\n",
    "    \n",
    "    # Type and text\n",
    "    if isinstance(item, PictureItem):\n",
    "        item_type, text = 'figure', '[Figure]'\n",
    "    elif isinstance(item, TableItem):\n",
    "        item_type, text = 'table', item.export_to_markdown(doc=doc)\n",
    "    elif isinstance(item, TextItem):\n",
    "        label = str(item.label).lower() if hasattr(item, 'label') else ''\n",
    "        item_type = ('heading' if 'section' in label or 'title' in label else\n",
    "                    'code' if 'code' in label else\n",
    "                    'equation' if 'formula' in label else\n",
    "                    'list' if 'list' in label else 'paragraph')\n",
    "        text = item.text or \"\"\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Build block\n",
    "    block = {\n",
    "        \"block_id\": block_id,\n",
    "        \"type\": item_type,\n",
    "        \"text\": text,\n",
    "        \"page\": page,\n",
    "        \"bbox\": bbox,\n",
    "        \"section_path\": \" > \".join(section_stack) if section_stack else \"root\",\n",
    "        \"pdf_hash\": ingest_plan[\"pdf_info\"][\"pdf_hash\"]\n",
    "    }\n",
    "    \n",
    "    # Type-specific fields\n",
    "    if item_type == \"heading\":\n",
    "        block[\"heading_level\"] = level\n",
    "        if text and len(text) > 3:\n",
    "            if level <= len(section_stack):\n",
    "                section_stack = section_stack[:level-1]\n",
    "            section_stack.append(text)\n",
    "    elif item_type == \"figure\":\n",
    "        block[\"caption\"] = item.caption_text(doc=doc)\n",
    "        block[\"figure_id\"] = str(item.self_ref)\n",
    "    elif item_type == \"table\":\n",
    "        block[\"caption\"] = item.caption_text(doc=doc)\n",
    "        block[\"table_id\"] = str(item.self_ref)\n",
    "    elif item_type == \"code\":\n",
    "        block[\"code_language\"] = getattr(item, 'code_language', 'unknown')\n",
    "    elif item_type == \"equation\":\n",
    "        block[\"latex\"] = getattr(item, 'latex', None)\n",
    "    \n",
    "    blocks.append(block)\n",
    "    block_id += 1\n",
    "\n",
    "# Augment missing captions\n",
    "for i, block in enumerate(blocks):\n",
    "    if block['type'] not in ['figure', 'table'] or block.get('caption'):\n",
    "        continue\n",
    "    for j in range(i-1, max(i-20, -1), -1):\n",
    "        prev = blocks[j]\n",
    "        if prev['page'] != block['page']:\n",
    "            continue\n",
    "        if prev['type'] == 'heading':\n",
    "            block['caption'] = prev['text']\n",
    "            break\n",
    "\n",
    "# Save JSONL\n",
    "with open(output_dir / \"docling_blocks.jsonl\", 'w') as f:\n",
    "    for block in blocks:\n",
    "        f.write(json.dumps(block, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Stats\n",
    "fig_tab = [b for b in blocks if b['type'] in ['figure', 'table']]\n",
    "with_caps = [b for b in fig_tab if b.get('caption')]\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(blocks)} blocks\")\n",
    "print(f\"✓ Captions: {len(with_caps)}/{len(fig_tab)} ({len(with_caps)/len(fig_tab)*100:.1f}%)\")\n",
    "print(f\"✓ Saved: {output_dir / 'docling_blocks.jsonl'}\")\n",
    "print(\"\\nREADY FOR PHASE B: CHUNKING EXPERIMENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e203459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 19:19:12,548 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-17 19:19:12,621 - INFO - Going to convert document batch...\n",
      "2025-10-17 19:19:12,627 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 4f2edc0f7d9bb60b38ebfecf9a2609f5\n",
      "2025-10-17 19:19:12,642 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-10-17 19:19:12,643 - INFO - Accelerator device: 'mps'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting fintbx_ex.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 19:19:13,893 - INFO - Accelerator device: 'mps'\n",
      "2025-10-17 19:19:14,389 - INFO - Processing document fintbx_ex.pdf\n",
      "2025-10-17 19:19:38,790 - INFO - Finished converting document fintbx_ex.pdf in 26.25 sec.\n",
      "2025-10-17 19:19:38,791 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,797 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,798 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,800 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,800 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,800 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,801 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,802 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,803 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,803 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,804 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,804 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n",
      "2025-10-17 19:19:38,805 - WARNING - Usage of TableItem.export_to_markdown() without `doc` argument is deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting blocks with metadata...\n",
      "✓ Extracted 907 blocks\n",
      "✓ Saved JSONL: outputs/phase_a/artifacts/docling_blocks.jsonl\n",
      "\n",
      "============================================================\n",
      "STATISTICS:\n",
      "============================================================\n",
      "Total blocks: 907\n",
      "Figures: 8\n",
      "Tables: 13\n",
      "Captions: 1/21 (4.8%)\n",
      "\n",
      "Sample blocks with metadata:\n",
      "\n",
      "  Block 0:\n",
      "    Type: heading\n",
      "    Page: 1\n",
      "    BBox: {'x0': 54.0, 'y0': 748.764, 'x1': 266.256, 'y1': 686.778}\n",
      "    Section: root\n",
      "    Text: Financial Toolbox™ User's Guide...\n",
      "\n",
      "  Block 1:\n",
      "    Type: figure\n",
      "    Page: 1\n",
      "    BBox: {'x0': 112.21453857421875, 'y0': 595.7596588134766, 'x1': 494.5715026855469, 'y1': 257.2275390625}\n",
      "    Section: Financial Toolbox™ User's Guide\n",
      "    Caption: \n",
      "    Text: [Figure]...\n",
      "\n",
      "  Block 2:\n",
      "    Type: heading\n",
      "    Page: 1\n",
      "    BBox: {'x0': 143.50362630178907, 'y0': 193.67892024425078, 'x1': 486.9023893226224, 'y1': 119.43735835460234}\n",
      "    Section: Financial Toolbox™ User's Guide\n",
      "    Text: MATLAB®...\n",
      "\n",
      "  Block 3:\n",
      "    Type: figure\n",
      "    Page: 1\n",
      "    BBox: {'x0': 436.68817138671875, 'y0': 66.8349609375, 'x1': 584.9988403320312, 'y1': 36.52667236328125}\n",
      "    Section: MATLAB®\n",
      "    Caption: \n",
      "    Text: [Figure]...\n",
      "\n",
      "  Block 4:\n",
      "    Type: figure\n",
      "    Page: 1\n",
      "    BBox: {'x0': 47.74920654296875, 'y0': 56.5421142578125, 'x1': 98.32001495361328, 'y1': 43.9503173828125}\n",
      "    Section: MATLAB®\n",
      "    Caption: \n",
      "    Text: [Figure]...\n"
     ]
    }
   ],
   "source": [
    "# %% PHASE A FINAL - Ready for Phase B Chunking\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.types.doc import TextItem, TableItem, PictureItem\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Parse\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE A: PARSE & NORMALIZE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pdf_path = Path(\"/Users/RiyanshiKedia/Documents/GitHub/project-aurelia/DATA/fintbx.pdf\")\n",
    "output_dir = Path(\"outputs/phase_a/artifacts\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(str(pdf_path))\n",
    "doc = result.document\n",
    "\n",
    "# Extract blocks\n",
    "blocks = []\n",
    "block_id = 0\n",
    "section_stack = []\n",
    "\n",
    "for item, level in doc.iterate_items():\n",
    "    \n",
    "    # Metadata\n",
    "    page, bbox = None, None\n",
    "    if hasattr(item, 'prov') and item.prov:\n",
    "        prov = item.prov[0]\n",
    "        page = getattr(prov, 'page_no', None)\n",
    "        if hasattr(prov, 'bbox'):\n",
    "            b = prov.bbox\n",
    "            bbox = {\"x0\": float(b.l), \"y0\": float(b.t), \"x1\": float(b.r), \"y1\": float(b.b)}\n",
    "    \n",
    "    # Type and text\n",
    "    if isinstance(item, PictureItem):\n",
    "        item_type, text = 'figure', '[Figure]'\n",
    "    elif isinstance(item, TableItem):\n",
    "        item_type, text = 'table', item.export_to_markdown(doc=doc)\n",
    "    elif isinstance(item, TextItem):\n",
    "        label = str(item.label).lower() if hasattr(item, 'label') else ''\n",
    "        item_type = ('heading' if 'section' in label or 'title' in label else\n",
    "                    'code' if 'code' in label else\n",
    "                    'equation' if 'formula' in label else\n",
    "                    'list' if 'list' in label else 'paragraph')\n",
    "        text = item.text or \"\"\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Build block\n",
    "    block = {\n",
    "        \"block_id\": block_id,\n",
    "        \"type\": item_type,\n",
    "        \"text\": text,\n",
    "        \"page\": page,\n",
    "        \"bbox\": bbox,\n",
    "        \"section_path\": \" > \".join(section_stack) if section_stack else \"root\",\n",
    "        \"pdf_hash\": ingest_plan[\"pdf_info\"][\"pdf_hash\"]\n",
    "    }\n",
    "    \n",
    "    # Type-specific fields\n",
    "    if item_type == \"heading\":\n",
    "        block[\"heading_level\"] = level\n",
    "        if text and len(text) > 3:\n",
    "            if level <= len(section_stack):\n",
    "                section_stack = section_stack[:level-1]\n",
    "            section_stack.append(text)\n",
    "    elif item_type == \"figure\":\n",
    "        block[\"caption\"] = item.caption_text(doc=doc)\n",
    "        block[\"figure_id\"] = str(item.self_ref)\n",
    "    elif item_type == \"table\":\n",
    "        block[\"caption\"] = item.caption_text(doc=doc)\n",
    "        block[\"table_id\"] = str(item.self_ref)\n",
    "    elif item_type == \"code\":\n",
    "        block[\"code_language\"] = getattr(item, 'code_language', 'unknown')\n",
    "    elif item_type == \"equation\":\n",
    "        block[\"latex\"] = getattr(item, 'latex', None)\n",
    "    \n",
    "    blocks.append(block)\n",
    "    block_id += 1\n",
    "\n",
    "# Augment missing captions\n",
    "for i, block in enumerate(blocks):\n",
    "    if block['type'] not in ['figure', 'table'] or block.get('caption'):\n",
    "        continue\n",
    "    for j in range(i-1, max(i-20, -1), -1):\n",
    "        prev = blocks[j]\n",
    "        if prev['page'] != block['page']:\n",
    "            continue\n",
    "        if prev['type'] == 'heading':\n",
    "            block['caption'] = prev['text']\n",
    "            break\n",
    "\n",
    "# Save JSONL\n",
    "with open(output_dir / \"docling_blocks.jsonl\", 'w') as f:\n",
    "    for block in blocks:\n",
    "        f.write(json.dumps(block, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Stats\n",
    "fig_tab = [b for b in blocks if b['type'] in ['figure', 'table']]\n",
    "with_caps = [b for b in fig_tab if b.get('caption')]\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(blocks)} blocks\")\n",
    "print(f\"✓ Captions: {len(with_caps)}/{len(fig_tab)} ({len(with_caps)/len(fig_tab)*100:.1f}%)\")\n",
    "print(f\"✓ Saved: {output_dir / 'docling_blocks.jsonl'}\")\n",
    "print(\"\\nREADY FOR PHASE B: CHUNKING EXPERIMENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb074b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
